{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification des documents du procès des groupes américains du tabac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réalisé par: Nasreddine Menacer<br>\n",
    "Université de Rouen / Master 2 SD-2018/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contexte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Analyse des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le gouvernement américain a attaqué en justice cinq grands groupes américains du tabac pour avoir amassé d'importants bénéfices en mentant sur les dangers de la cigarette. Le cigarettiers  se sont entendus dès 1953, pour \"mener ensemble une vaste campagne de relations publiques afin de contrer les preuves de plus en plus manifestes d'un lien entre la consommation de tabac et des maladies graves\".\n",
    "Dans ce procès 14 millions de documents ont été collectés et numérisés. Afin de faciliter l'exploitation de ces documents par les avocats, vous êtes en charge de mettre en place une classification automatique des types de documents.\n",
    "Un échantillon aléatoire des documents a été collecté et des opérateurs ont classé les documents dans des répertoires correspondant aux classes de documents : lettres, rapports, notes, email, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement des Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Premièrement je vais charger le fichier 'Tobacco3482.csv' qui contient la classe de  chaque document,\n",
    "définies par des opérateurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data= pd.read_csv('Tobacco3482.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Je vais ensuite faire une analyse statistique des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb8AAAEKCAYAAACCFFu0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMzMrHCc/MzMrnP8G0h+2ADZgVNIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data=data,y='label') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que pour des techniques de classification et de traitement automatique de texte, le nombre des données est faible, et qu'il y a seulement quatre étiquettes plus présentes que les autres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- J'ai aussi effectué une analyse descriptive qui va permettre d'étudier la distribution des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>News/1002402255-a.jpg</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>Email/2085725088c.jpg</td>\n",
       "      <td>Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>Email/2081499563a.jpg</td>\n",
       "      <td>Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2607</th>\n",
       "      <td>News/2083781257.jpg</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>Form/2083849128.jpg</td>\n",
       "      <td>Form</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>Email/528954681+-4681.jpg</td>\n",
       "      <td>Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2408</th>\n",
       "      <td>Memo/96018853_8858.jpg</td>\n",
       "      <td>Memo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3319</th>\n",
       "      <td>Scientific/2057342922_2947.jpg</td>\n",
       "      <td>Scientific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3206</th>\n",
       "      <td>Resume/50638712-8712.jpg</td>\n",
       "      <td>Resume</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2727</th>\n",
       "      <td>Note/2030053173.jpg</td>\n",
       "      <td>Note</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            img_path       label\n",
       "2467           News/1002402255-a.jpg        News\n",
       "628            Email/2085725088c.jpg       Email\n",
       "471            Email/2081499563a.jpg       Email\n",
       "2607             News/2083781257.jpg        News\n",
       "1101             Form/2083849128.jpg        Form\n",
       "786        Email/528954681+-4681.jpg       Email\n",
       "2408          Memo/96018853_8858.jpg        Memo\n",
       "3319  Scientific/2057342922_2947.jpg  Scientific\n",
       "3206        Resume/50638712-8712.jpg      Resume\n",
       "2727             Note/2030053173.jpg        Note"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = pd.read_csv('Tobacco3482.csv', sep = \",\")\n",
    "classes.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- J'ai aussi vérifier s'il y a des étiquettes manquantes, des incohérence entre la classe réelle du document et la classe notée "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les étiquettes manquantes : 0.0\n",
      "Les documents mal classé : 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Les étiquettes manquantes :\", 1.0 - classes.shape[0] / classes.dropna().shape[0])\n",
    "s = 0\n",
    "for i in range(classes.shape[0]):\n",
    "    s += classes[\"img_path\"][i].split(\"/\")[0] == classes[\"label\"][i]\n",
    "print(\"Les documents mal classé :\", classes.shape[0] - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a la certitude qu'il n'y a aucun document mal classé, et qu'il n'y a aucune étiquette manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pour avoir une idée des données qu'on vas manipuler j'ai visualisé un echentillont du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text          label\n",
      "0  A Mpertant as yar\\nsesiye teaetered cabiieess....  Advertisement\n",
      "1  TE che fitm\\n\\nm66400 7127\\n\\nKOOLS are the on...  Advertisement\n",
      "2  so ARN Rr nr\\n\\nBWR Ga ||\\n\\nVending Operators...  Advertisement\n",
      "3  MARCH 24,19 VO — 3. Tersrearep\\n\\n \\n\\n‘ yi il...  Advertisement\n",
      "4  ~\\n\\nSpend a milder moment qs\\nwith Raleigh.\\n...  Advertisement\n"
     ]
    }
   ],
   "source": [
    "data_txt=[]\n",
    "data_txt=data\n",
    "\n",
    "NB = data_txt.shape[0]\n",
    "for i in range (NB):\n",
    "    A = data_txt.get_value(i, 'img_path')\n",
    "    data_txt.set_value(i, 'img_path', 'Tobacco3482-OCR/'+A)\n",
    "    data_txt.set_value(i, 'img_path', data_txt.get_value(i, 'img_path').split('.jpg')[0]+'.txt')\n",
    "    data_txt.set_value(i, 'img_path',open(data_txt.get_value(i, 'img_path'), \"r\",encoding=\"utf8\").read())\n",
    "data_txt.columns = ['text','label']\n",
    "print(data_txt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut visualiser ici les quatre premiers textes (une partie du texte), et l'étiquette associée a celui-ci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Machine learniing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j'ai choisie ici d'appliquer deux mèthodes de machine learning, et donc traiter le texte, pour essayer d'entrainer les classifieurs et essayer de prédire la classe d'un document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1-Bag of Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour appliquer la méthode de bag of word, les documents doivent être transformés en vecteurs. Pour cela j'ai transformer les documents en vecteurs et je les est encodés en sac de mots en utilisant la fonction CountVectorizer de Sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant ça j'ai dévisé les données en trois parties, données d'entrainement, données de test, et données de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_app, y_train, y_app = train_test_split(data['text'], data['label'], test_size=0.4)\n",
    "X_test,X_dev,y_test,y_dev=train_test_split(X_app, y_app, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j'ai pris 60% des données comme données d'entrainement, et les 40% qui reste sont dévisés en deux pour servir de données de teste et de validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size= 2089 X_test data size= 696 dev data size= 697\n"
     ]
    }
   ],
   "source": [
    "print('train data size=',X_train.shape[0],'X_test data size=',X_test.shape[0],'dev data size=',X_dev.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=2000)\n",
    "vectorizer.fit(X_train)\n",
    "X_train_counts = vectorizer.transform(X_train)\n",
    "X_dev_counts = vectorizer.transform(X_dev)\n",
    "X_test_counts = vectorizer.transform(X_test)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- J'ai ensuite appliqué la fonction GridSearsh sur un classifieur bayésien naïf, pour trouver le meilleur paramètre alpha, avec une cross validation=5, la précision du classifieur est la moyenne des précision de chaque opération.\n",
    "- Le principe de ce classifieur est d'utiliser une hypothèse d'indépendence entre les caractéristiques des classes pour estimer une probabilité associée à l'appartenance à une classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rang 1\n",
      "Alpha : 1.0\n",
      "La précision moyenne : 0.722\n",
      "\n",
      "Rang 2\n",
      "Alpha : 2.0\n",
      "La précision moyenne : 0.717\n",
      "\n",
      "Rang 3\n",
      "Alpha : 3.0\n",
      "La précision moyenne : 0.711\n",
      "\n",
      "Rang 4\n",
      "Alpha : 4.0\n",
      "La précision moyenne : 0.707\n",
      "\n",
      "Rang 5\n",
      "Alpha : 5.0\n",
      "La précision moyenne : 0.703\n",
      "\n",
      "Rang 6\n",
      "Alpha : 7.0\n",
      "La précision moyenne : 0.683\n",
      "\n",
      "Rang 7\n",
      "Alpha : 8.0\n",
      "La précision moyenne : 0.677\n",
      "\n",
      "Rang 8\n",
      "Alpha : 9.0\n",
      "La précision moyenne : 0.673\n",
      "\n",
      "Rang 9\n",
      "Alpha : 10.0\n",
      "La précision moyenne : 0.67\n",
      "\n",
      "Rang 10\n",
      "Alpha : 0.0\n",
      "La précision moyenne : 0.644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = {'alpha' : [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 7.0, 8.0, 9.0, 10.0]}\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "grid_search_clf = GridSearchCV(nb_classifier, parameters, cv=5, return_train_score=True)\n",
    "grid_search_clf.fit(X_train_counts, y_train)\n",
    "res = grid_search_clf.cv_results_\n",
    "\n",
    "for i in range(1,11):\n",
    "    print('Rang', i)\n",
    "    ind = np.where(res['rank_test_score'] == i)\n",
    "    print('Alpha : {}'.format(res['params'][ind[0][0]]['alpha']))\n",
    "    print('La précision moyenne : {}\\n'.format(round(res['mean_test_score'][ind][0], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprés dix valuers de alpha testés par GridSearch, on remparque que la valeur 1 donne la meilleur precision, je vais utiliser cette valeur donc pour construire un classifiur bayésien, l'entrainé et le testé sur les données de teste et de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La précision sur les données d'entrainement est :  0.8276687410244136\n",
      "La précision sur les données de validation est :  0.7187948350071736\n",
      "La précision sur les données de test est :  0.7083333333333334\n"
     ]
    }
   ],
   "source": [
    "nb_classifier = MultinomialNB(alpha=1.)\n",
    "nb_classifier.fit(X_train_counts, y_train)\n",
    "\n",
    "pred_train = nb_classifier.predict(X_train_counts)\n",
    "pred_dev = nb_classifier.predict(X_dev_counts)\n",
    "pred_test = nb_classifier.predict(X_test_counts)\n",
    "\n",
    "print(\"La précision sur les données d'entrainement est : \", metrics.accuracy_score(y_train, pred_train))\n",
    "print(\"La précision sur les données de validation est : \", metrics.accuracy_score(y_dev, pred_dev))\n",
    "print(\"La précision sur les données de test est : \", metrics.accuracy_score(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2-TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_transformer = TfidfTransformer().fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_dev_tf = tf_transformer.transform(X_dev_counts)\n",
    "X_test_tf = tf_transformer.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De meme que pour la methode de bag of word, je vais utiliser, le classifieur bayésien, créé avec un alpha=1, pour calculer la precision sur chaque jeux de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La précision sur les données d'entrainement est:  0.7510770703685974\n",
      "La précision sur les données de validation est :  0.648493543758967\n",
      "La précision sur les données de test est :  0.6609195402298851\n"
     ]
    }
   ],
   "source": [
    "nb_classifier.fit(X_train_tf, y_train)\n",
    "\n",
    "pred_train_tf = nb_classifier.predict(X_train_tf)\n",
    "pred_dev_tf = nb_classifier.predict(X_dev_tf)\n",
    "pred_test_tf = nb_classifier.predict(X_test_tf)\n",
    "\n",
    "print(\"La précision sur les données d'entrainement est: \", metrics.accuracy_score(y_train, pred_train_tf))\n",
    "print(\"La précision sur les données de validation est : \", metrics.accuracy_score(y_dev, pred_dev_tf))\n",
    "print(\"La précision sur les données de test est : \", metrics.accuracy_score(y_test, pred_test_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - MLP with BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai choisie dans cette partie d'implémenter un MLP ( multi layer perceptrant), en utilisant un GridSearch  et  les données issus de la methode bag of word, puisque c'est elle qui nous à donné de meilleur résultat de pecision, le GridSearch avec une cross validation vas nous donner les meilleur paramètres à utiliser dans notre modèle, avec 10 itération j'ai choisie entre 1 et 5 neurone dans la couche, pour la couche d'activation entre 'tanh' et 'relu', et entre 'adam' et 'sgd' pour le solveur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=10, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'hidden_layer_sizes': [(1,), (2,), (3,), (4,)], 'activation': ['tanh', 'relu'], 'solver': ['sgd', 'adam'], 'alpha': [1, 5], 'learning_rate': ['constant', 'adaptive']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(max_iter=10)\n",
    "\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(i,) for i in range(1,5)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [1,5],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "clf.fit(X_train_counts, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ici les meilleur paramatères sont affichés "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (4,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.066 (+/-0.049) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (1,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.216 (+/-0.181) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (1,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.137 (+/-0.065) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (1,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.256 (+/-0.023) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (1,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.177 (+/-0.127) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (2,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.286 (+/-0.250) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (2,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.159 (+/-0.066) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (2,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.242 (+/-0.081) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (2,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.128 (+/-0.120) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (3,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.312 (+/-0.198) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (3,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.213 (+/-0.042) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (3,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.313 (+/-0.198) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (3,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.217 (+/-0.084) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (4,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.405 (+/-0.111) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (4,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.240 (+/-0.099) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (4,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.463 (+/-0.060) for {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (4,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.137 (+/-0.051) for {'activation': 'tanh', 'alpha': 5, 'hidden_layer_sizes': (1,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.168 (+/-0.092) for {'activation': 'tanh', 'alpha': 5, 'hidden_layer_sizes': (1,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.163 (+/-0.136) for {'activation': 'tanh', 'alpha': 5, 'hidden_layer_sizes': (1,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.170 (+/-0.114) for {'activation': 'tanh', 'alpha': 5, 'hidden_layer_sizes': (1,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.134 (+/-0.052) for {'activation': 'tanh', 'alpha': 5, 'hidden_layer_sizes': (2,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.213 (+/-0.089) for {'activation': 'tanh', 'alpha': 5, 'hidden_layer_sizes': (2,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.155 (+/-0.068) for {'activation': 'tanh', 'alpha': 5, 'hidden_layer_sizes': (2,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.263 (+/-0.075) for {'activation': 'tanh', 'alpha': 5, 'hidden_layer_sizes': (2,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.220 (+/-0.144) for {'activation': 'tanh', 'alpha': 5, 'hidden_layer_sizes': (3,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.353 (+/-0.073) for {'activation': 'tanh', 'alpha': 5, 'hidden_layer_sizes': (3,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.218 (+/-0.172) for {'activation': 'tanh', 'alpha': 5, 'hidden_layer_sizes': (3,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.331 (+/-0.275) for {'activation': 'tanh', 'alpha': 5, 'hidden_layer_sizes': (3,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.234 (+/-0.018) for {'activation': 'tanh', 'alpha': 5, 'hidden_layer_sizes': (4,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.408 (+/-0.050) for {'activation': 'tanh', 'alpha': 5, 'hidden_layer_sizes': (4,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.237 (+/-0.100) for {'activation': 'tanh', 'alpha': 5, 'hidden_layer_sizes': (4,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.442 (+/-0.131) for {'activation': 'tanh', 'alpha': 5, 'hidden_layer_sizes': (4,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.135 (+/-0.099) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (1,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.098 (+/-0.115) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (1,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.071 (+/-0.023) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (1,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.120 (+/-0.084) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (1,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.187 (+/-0.077) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (2,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.188 (+/-0.106) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (2,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.139 (+/-0.109) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (2,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.174 (+/-0.139) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (2,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.187 (+/-0.066) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (3,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.325 (+/-0.110) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (3,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.143 (+/-0.108) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (3,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.353 (+/-0.153) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (3,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.161 (+/-0.030) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (4,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.314 (+/-0.297) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (4,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.199 (+/-0.026) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (4,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.320 (+/-0.179) for {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (4,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.106 (+/-0.096) for {'activation': 'relu', 'alpha': 5, 'hidden_layer_sizes': (1,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.178 (+/-0.120) for {'activation': 'relu', 'alpha': 5, 'hidden_layer_sizes': (1,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.134 (+/-0.072) for {'activation': 'relu', 'alpha': 5, 'hidden_layer_sizes': (1,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.162 (+/-0.114) for {'activation': 'relu', 'alpha': 5, 'hidden_layer_sizes': (1,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.177 (+/-0.016) for {'activation': 'relu', 'alpha': 5, 'hidden_layer_sizes': (2,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.165 (+/-0.076) for {'activation': 'relu', 'alpha': 5, 'hidden_layer_sizes': (2,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.162 (+/-0.037) for {'activation': 'relu', 'alpha': 5, 'hidden_layer_sizes': (2,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.273 (+/-0.227) for {'activation': 'relu', 'alpha': 5, 'hidden_layer_sizes': (2,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.141 (+/-0.031) for {'activation': 'relu', 'alpha': 5, 'hidden_layer_sizes': (3,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.300 (+/-0.094) for {'activation': 'relu', 'alpha': 5, 'hidden_layer_sizes': (3,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.201 (+/-0.028) for {'activation': 'relu', 'alpha': 5, 'hidden_layer_sizes': (3,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.363 (+/-0.157) for {'activation': 'relu', 'alpha': 5, 'hidden_layer_sizes': (3,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.171 (+/-0.149) for {'activation': 'relu', 'alpha': 5, 'hidden_layer_sizes': (4,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.371 (+/-0.174) for {'activation': 'relu', 'alpha': 5, 'hidden_layer_sizes': (4,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.117 (+/-0.104) for {'activation': 'relu', 'alpha': 5, 'hidden_layer_sizes': (4,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.433 (+/-0.083) for {'activation': 'relu', 'alpha': 5, 'hidden_layer_sizes': (4,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "je vais utiliser ensuite ces paramètres pour essayer de prédir la classe de chaque documents des données de test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true, y_pred = y_test , clf.predict(X_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-Analyse des Performances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayse Bag Of  Word  \n",
      "\n",
      "La précision sur les données d'entrainement est :  0.8276687410244136\n",
      "La précision sur les données de validation est :  0.7187948350071736\n",
      "La précision sur les données de test est :  0.7083333333333334\n",
      "\n",
      "\n",
      " Matrice de Classification \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Advertisement       0.65      0.61      0.63        46\n",
      "        Email       0.90      0.95      0.93       118\n",
      "         Form       0.73      0.71      0.72        87\n",
      "       Letter       0.76      0.65      0.70       120\n",
      "         Memo       0.60      0.69      0.65       118\n",
      "         News       0.63      0.79      0.70        39\n",
      "         Note       0.34      0.35      0.35        31\n",
      "       Report       0.62      0.56      0.59        52\n",
      "       Resume       1.00      1.00      1.00        26\n",
      "   Scientific       0.65      0.58      0.61        59\n",
      "\n",
      "  avg / total       0.71      0.71      0.71       696\n",
      "\n",
      "Matrice de Confusion Bag Of Word\n",
      "[[ 28   2   4   1   3   1   6   1   0   0]\n",
      " [  0 112   0   2   4   0   0   0   0   0]\n",
      " [  6   0  62   4   4   0   8   1   0   2]\n",
      " [  0   2   2  78  19   6   1   9   0   3]\n",
      " [  3   6   2  11  82   3   4   3   0   4]\n",
      " [  5   0   0   1   2  31   0   0   0   0]\n",
      " [  0   2   3   0  12   1  11   1   0   1]\n",
      " [  0   0   1   3   5   6   0  29   0   8]\n",
      " [  0   0   0   0   0   0   0   0  26   0]\n",
      " [  1   0  11   2   5   1   2   3   0  34]]\n"
     ]
    }
   ],
   "source": [
    "print('Naive Bayse Bag Of  Word  \\n')\n",
    "print(\"La précision sur les données d'entrainement est : \", metrics.accuracy_score(y_train, pred_train))\n",
    "print(\"La précision sur les données de validation est : \", metrics.accuracy_score(y_dev, pred_dev))\n",
    "print(\"La précision sur les données de test est : \", metrics.accuracy_score(y_test, pred_test))\n",
    "print('\\n\\n Matrice de Classification \\n')\n",
    "print(classification_report(y_test, pred_test))\n",
    "print('Matrice de Confusion Bag Of Word')\n",
    "print(confusion_matrix(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF \n",
      "\n",
      "La précision sur les données d'entrainement es:  0.7510770703685974\n",
      "La précision sur les données de validation est :  0.648493543758967\n",
      "La précision sur les données de test est :  0.6609195402298851\n",
      "\n",
      "\n",
      " Matrice de Classification \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Advertisement       0.79      0.41      0.54        46\n",
      "        Email       0.93      0.95      0.94       118\n",
      "         Form       0.67      0.74      0.70        87\n",
      "       Letter       0.61      0.71      0.65       120\n",
      "         Memo       0.46      0.89      0.61       118\n",
      "         News       0.86      0.49      0.62        39\n",
      "         Note       1.00      0.06      0.12        31\n",
      "       Report       1.00      0.02      0.04        52\n",
      "       Resume       1.00      0.96      0.98        26\n",
      "   Scientific       0.74      0.47      0.58        59\n",
      "\n",
      "  avg / total       0.74      0.66      0.63       696\n",
      "\n",
      "\n",
      "Matrice de Confusion TF-IDF\n",
      "[[ 19   0  11   6  10   0   0   0   0   0]\n",
      " [  0 112   0   1   5   0   0   0   0   0]\n",
      " [  0   1  64   4  16   0   0   0   0   2]\n",
      " [  0   1   0  85  33   0   0   0   0   1]\n",
      " [  0   4   2   7 105   0   0   0   0   0]\n",
      " [  5   0   0  11   4  19   0   0   0   0]\n",
      " [  0   3  11   0  13   1   2   0   0   1]\n",
      " [  0   0   0  18  26   1   0   1   0   6]\n",
      " [  0   0   0   1   0   0   0   0  25   0]\n",
      " [  0   0   8   7  15   1   0   0   0  28]]\n"
     ]
    }
   ],
   "source": [
    "print('TF-IDF \\n')\n",
    "print(\"La précision sur les données d'entrainement es: \", metrics.accuracy_score(y_train, pred_train_tf))\n",
    "print(\"La précision sur les données de validation est : \", metrics.accuracy_score(y_dev, pred_dev_tf))\n",
    "print(\"La précision sur les données de test est : \", metrics.accuracy_score(y_test, pred_test_tf))\n",
    "print('\\n\\n Matrice de Classification \\n')\n",
    "print(classification_report(y_test, pred_test_tf))\n",
    "print()\n",
    "print('Matrice de Confusion TF-IDF')\n",
    "print(confusion_matrix(y_test, pred_test_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MLP Classifier with TF-IDF representation \n",
      "\n",
      "Resultats sur les données de test:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Advertisement       0.00      0.00      0.00        46\n",
      "        Email       0.32      0.99      0.49       118\n",
      "         Form       0.90      0.40      0.56        87\n",
      "       Letter       0.58      0.69      0.63       120\n",
      "         Memo       0.00      0.00      0.00       118\n",
      "         News       0.00      0.00      0.00        39\n",
      "         Note       0.07      0.06      0.07        31\n",
      "       Report       0.17      0.29      0.21        52\n",
      "       Resume       0.00      0.00      0.00        26\n",
      "   Scientific       0.67      0.34      0.45        59\n",
      "\n",
      "  avg / total       0.34      0.39      0.32       696\n",
      "\n",
      "Matrice de Confusion MLP-BoW\n",
      "[[  0  22   0   2   0   0  17   4   0   1]\n",
      " [  0 117   0   1   0   0   0   0   0   0]\n",
      " [  0  43  35   1   0   0   6   1   0   1]\n",
      " [  0  27   0  83   0   0   0   8   0   2]\n",
      " [  0 102   1  10   0   0   1   2   0   2]\n",
      " [  0   5   0   0   0   0   0  34   0   0]\n",
      " [  0  28   0   0   0   0   2   1   0   0]\n",
      " [  0   9   1  23   0   0   0  15   0   4]\n",
      " [  0   0   0   5   0   0   0  21   0   0]\n",
      " [  0  11   2  19   0   0   4   3   0  20]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(' MLP Classifier with TF-IDF representation \\n')\n",
    "from sklearn.metrics import classification_report\n",
    "print('Resultats sur les données de test:')\n",
    "print(classification_report(y_true, y_pred))\n",
    "print('Matrice de Confusion MLP-BoW')\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce travail j'ai comparé entre deux methodes, la première cosistait à entrainer un classifieur bayésien naif, sur des données transformées par un Bag Of Word, et un TF-IDF, la deuxième methode consiste à utiliser un réseau de neurones dans mon cas un MLP, pour classer les meme donnnées. l'utilisation d'un classifieur bayesien,est  réputé pour etre non performant dans le cas où les mots son dépendant entre eux, mais puisque c'est simple à implémenter et rapide à exécuter j'ai choisie de le tester, et puis le comparer avec un reseau de neurone qui sont réputés pour etre beaucoup plus raubuste et voir ce que ça donne.\n",
    "\n",
    "J'ai remarque que le modèle bayèsien nous donne une précision de 71% dans le cas où les données sont transformées avec un bag of word, et 64% dans le cas où il sont transformées avec un TF-IDF\n",
    "Tandis qu'avec le reseau de neurones la précision sur les données de test est de 34%.\n",
    "En regardant la matrice de Confusion des deux approches, on remarque que la méthode bayésiène est équilibré , contairement à la matrice du reseau de neurone qui semble pas équilibré, puisque nombreuses classes sont confondus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pistes d'améliorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l'idée que j'ai eu et que j'ai appliqué durant ce travail et de combiner les approches, mais on pourrait aller plus loin, puisque de nos jours les CNN, sont entrain d'ateindres des résultats trés satisfaisants,en terme de reconnaissance d'image, et puisque on à déjas les images des documents scanés, on aura pas besoin du texte obtenu à l'aide d'un OCR et qui contient des erreurs d'interprétation, on donne en entrée à un reseau de neurones à convolution, nos images de documents qui va classer automatiquement, seul inconvénient c'est qu'il faut avoir une grande base de données d'apprentissage étiquettée. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
