{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification des documents du procès des groupes américains du tabac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réalisé par: Nasreddine Menacer<br>\n",
    "Université de Rouen / Master 2 SD-2018/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contexte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Analyse des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le gouvernement américain a attaqué en justice cinq grands groupes américains du tabac pour avoir amassé d'importants bénéfices en mentant sur les dangers de la cigarette. Le cigarettiers  se sont entendus dès 1953, pour \"mener ensemble une vaste campagne de relations publiques afin de contrer les preuves de plus en plus manifestes d'un lien entre la consommation de tabac et des maladies graves\".\n",
    "Dans ce procès 14 millions de documents ont été collectés et numérisés. Afin de faciliter l'exploitation de ces documents par les avocats, vous êtes en charge de mettre en place une classification automatique des types de documents.\n",
    "Un échantillon aléatoire des documents a été collecté et des opérateurs ont classé les documents dans des répertoires correspondant aux classes de documents : lettres, rapports, notes, email, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement des Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Premièrement je vais charger le fichier 'Tobacco3482.csv' qui contient la classe de  chaque document,\n",
    "définies par des opérateurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data= pd.read_csv('Tobacco3482.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Je vais ensuite faire une analyse statistique des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb8AAAEKCAYAAACCFFu0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMzMrHCc/MzMrnP8G0h+2ADZgVNIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data=data,y='label') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que pour des techniques de classification et de traitement automatique de texte, le nombre des données est faible, et qu'il y a seulement quatre étiquettes plus présentes que les autres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- J'ai aussi effectué une analyse descriptive qui va permettre d'étudier la distribution des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>Letter/502393490+-3490.jpg</td>\n",
       "      <td>Letter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>Letter/50004413_50004414.jpg</td>\n",
       "      <td>Letter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>Letter/50006758.jpg</td>\n",
       "      <td>Letter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2927</th>\n",
       "      <td>Report/505830872+-0875.jpg</td>\n",
       "      <td>Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3416</th>\n",
       "      <td>Scientific/50511224-1224.jpg</td>\n",
       "      <td>Scientific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>News/1003042513-b.jpg</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>Form/2030138503.jpg</td>\n",
       "      <td>Form</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>Form/2063330045.jpg</td>\n",
       "      <td>Form</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2901</th>\n",
       "      <td>Report/503958547_503958548.jpg</td>\n",
       "      <td>Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>Letter/504789202_504789204.jpg</td>\n",
       "      <td>Letter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            img_path       label\n",
       "1426      Letter/502393490+-3490.jpg      Letter\n",
       "1334    Letter/50004413_50004414.jpg      Letter\n",
       "1336             Letter/50006758.jpg      Letter\n",
       "2927      Report/505830872+-0875.jpg      Report\n",
       "3416    Scientific/50511224-1224.jpg  Scientific\n",
       "2478           News/1003042513-b.jpg        News\n",
       "910              Form/2030138503.jpg        Form\n",
       "1015             Form/2063330045.jpg        Form\n",
       "2901  Report/503958547_503958548.jpg      Report\n",
       "1490  Letter/504789202_504789204.jpg      Letter"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = pd.read_csv('Tobacco3482.csv', sep = \",\")\n",
    "classes.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- J'ai aussi vérifier s'il y a des étiquettes manquantes, des incohérence entre la classe réelle du document et la classe notée "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les étiquettes manquantes : 0.0\n",
      "Les documents mal classé : 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Les étiquettes manquantes :\", 1.0 - classes.shape[0] / classes.dropna().shape[0])\n",
    "s = 0\n",
    "for i in range(classes.shape[0]):\n",
    "    s += classes[\"img_path\"][i].split(\"/\")[0] == classes[\"label\"][i]\n",
    "print(\"Les documents mal classé :\", classes.shape[0] - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a la certitude qu'il n'y a aucun document mal classé, et qu'il n'y a aucune étiquette manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pour avoir une idée des données qu'on vas manipuler j'ai visualisé un echentillont du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text          label\n",
      "0  A Mpertant as yar\\nsesiye teaetered cabiieess....  Advertisement\n",
      "1  TE che fitm\\n\\nm66400 7127\\n\\nKOOLS are the on...  Advertisement\n",
      "2  so ARN Rr nr\\n\\nBWR Ga ||\\n\\nVending Operators...  Advertisement\n",
      "3  MARCH 24,19 VO — 3. Tersrearep\\n\\n \\n\\n‘ yi il...  Advertisement\n",
      "4  ~\\n\\nSpend a milder moment qs\\nwith Raleigh.\\n...  Advertisement\n"
     ]
    }
   ],
   "source": [
    "data_txt=[]\n",
    "data_txt=data\n",
    "\n",
    "NB = data_txt.shape[0]\n",
    "for i in range (NB):\n",
    "    A = data_txt.get_value(i, 'img_path')\n",
    "    data_txt.set_value(i, 'img_path', 'Tobacco3482-OCR/'+A)\n",
    "    data_txt.set_value(i, 'img_path', data_txt.get_value(i, 'img_path').split('.jpg')[0]+'.txt')\n",
    "    data_txt.set_value(i, 'img_path',open(data_txt.get_value(i, 'img_path'), \"r\",encoding=\"utf8\").read())\n",
    "data_txt.columns = ['text','label']\n",
    "print(data_txt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut visualiser ici les quatre premiers textes (une partie du texte), et l'étiquette associée a celui-ci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Machine learniing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j'ai choisie ici d'appliquer deux mèthodes de machine learning, et donc traiter le texte, pour essayer d'entrainer les classifieurs et essayer de prédire la classe d'un document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1-Bag of Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour appliquer la méthode de bag of word, les documents doivent être transformés en vecteurs. Pour cela j'ai transformer les documents en vecteurs et je les est encodés en sac de mots en utilisant la fonction CountVectorizer de Sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant ça j'ai dévisé les données en trois parties, données d'entrainement, données de test, et données de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_app, y_train, y_app = train_test_split(data['text'], data['label'], test_size=0.4)\n",
    "X_test,X_dev,y_test,y_dev=train_test_split(X_app, y_app, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j'ai pris 60% des données comme données d'entrainement, et les 40% qui reste sont dévisés en deux pour servir de données de teste et de validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size= 2089 X_test data size= 696 dev data size= 697\n"
     ]
    }
   ],
   "source": [
    "print('train data size=',X_train.shape[0],'X_test data size=',X_test.shape[0],'dev data size=',X_dev.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=2000)\n",
    "vectorizer.fit(X_train)\n",
    "X_train_counts = vectorizer.transform(X_train)\n",
    "X_dev_counts = vectorizer.transform(X_dev)\n",
    "X_test_counts = vectorizer.transform(X_test)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- J'ai ensuite appliqué la fonction GridSearsh sur un classifieur bayésien naïf, pour trouver le meilleur paramètre alpha, avec une cross validation=5, la précision du classifieur est la moyenne des précision de chaque opération.\n",
    "- Le principe de ce classifieur est d'utiliser une hypothèse d'indépendence entre les caractéristiques des classes pour estimer une probabilité associée à l'appartenance à une classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rang 1\n",
      "Alpha : 1.0\n",
      "La précision moyenne : 0.719\n",
      "\n",
      "Rang 2\n",
      "Alpha : 2.0\n",
      "La précision moyenne : 0.715\n",
      "\n",
      "Rang 3\n",
      "Alpha : 3.0\n",
      "La précision moyenne : 0.714\n",
      "\n",
      "Rang 4\n",
      "Alpha : 5.0\n",
      "La précision moyenne : 0.708\n",
      "\n",
      "Rang 5\n",
      "Alpha : 4.0\n",
      "La précision moyenne : 0.707\n",
      "\n",
      "Rang 6\n",
      "Alpha : 7.0\n",
      "La précision moyenne : 0.691\n",
      "\n",
      "Rang 7\n",
      "Alpha : 8.0\n",
      "La précision moyenne : 0.687\n",
      "\n",
      "Rang 8\n",
      "Alpha : 9.0\n",
      "La précision moyenne : 0.682\n",
      "\n",
      "Rang 9\n",
      "Alpha : 10.0\n",
      "La précision moyenne : 0.676\n",
      "\n",
      "Rang 10\n",
      "Alpha : 0.0\n",
      "La précision moyenne : 0.651\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = {'alpha' : [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 7.0, 8.0, 9.0, 10.0]}\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "grid_search_clf = GridSearchCV(nb_classifier, parameters, cv=5, return_train_score=True)\n",
    "grid_search_clf.fit(X_train_counts, y_train)\n",
    "res = grid_search_clf.cv_results_\n",
    "\n",
    "for i in range(1,11):\n",
    "    print('Rang', i)\n",
    "    ind = np.where(res['rank_test_score'] == i)\n",
    "    print('Alpha : {}'.format(res['params'][ind[0][0]]['alpha']))\n",
    "    print('La précision moyenne : {}\\n'.format(round(res['mean_test_score'][ind][0], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprés dix valuers de alpha testés par GridSearch, on remparque que la valeur 1 donne la meilleur precision, je vais utiliser cette valeur donc pour construire un classifiur bayésien, l'entrainé et le testé sur les données de teste et de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La précision sur les données d'entrainement est :  0.8353279080899952\n",
      "La précision sur les données de validation est :  0.7101865136298422\n",
      "La précision sur les données de test est :  0.7140804597701149\n"
     ]
    }
   ],
   "source": [
    "nb_classifier = MultinomialNB(alpha=1.)\n",
    "nb_classifier.fit(X_train_counts, y_train)\n",
    "\n",
    "pred_train = nb_classifier.predict(X_train_counts)\n",
    "pred_dev = nb_classifier.predict(X_dev_counts)\n",
    "pred_test = nb_classifier.predict(X_test_counts)\n",
    "\n",
    "print(\"La précision sur les données d'entrainement est : \", metrics.accuracy_score(y_train, pred_train))\n",
    "print(\"La précision sur les données de validation est : \", metrics.accuracy_score(y_dev, pred_dev))\n",
    "print(\"La précision sur les données de test est : \", metrics.accuracy_score(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2-TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_transformer = TfidfTransformer().fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_dev_tf = tf_transformer.transform(X_dev_counts)\n",
    "X_test_tf = tf_transformer.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De meme que pour la methode de bag of word, je vais utiliser, le classifieur bayésien, créé avec un alpha=1, pour calculer la precision sur chaque jeux de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La précision sur les données d'entrainement est:  0.7415031115366204\n",
      "La précision sur les données de validation est :  0.6556671449067432\n",
      "La précision sur les données de test est :  0.6422413793103449\n"
     ]
    }
   ],
   "source": [
    "nb_classifier.fit(X_train_tf, y_train)\n",
    "\n",
    "pred_train_tf = nb_classifier.predict(X_train_tf)\n",
    "pred_dev_tf = nb_classifier.predict(X_dev_tf)\n",
    "pred_test_tf = nb_classifier.predict(X_test_tf)\n",
    "\n",
    "print(\"La précision sur les données d'entrainement est: \", metrics.accuracy_score(y_train, pred_train_tf))\n",
    "print(\"La précision sur les données de validation est : \", metrics.accuracy_score(y_dev, pred_dev_tf))\n",
    "print(\"La précision sur les données de test est : \", metrics.accuracy_score(y_test, pred_test_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - MLP with BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai choisie dans cette partie d'implémenter un MLP ( multi layer perceptrant), avec les données issus de la methode bag of word, puisque c'est elle qui nous à donné de meilleur résultat de pecision, pour cela j'ai utilisé une couche d'activation 'relu', un alpha=1, un solveur de type 'verbose' et un batch size de taille 50, normalement c'est paramètre devrait etre calculé avec la fonction GridSearch, avec une cross validation mais par manque de ressources matériels, ça prend beaucoup de temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.02131435\n",
      "Iteration 2, loss = 1.64572152\n",
      "Iteration 3, loss = 1.20849408\n",
      "Iteration 4, loss = 1.00649682\n",
      "Iteration 5, loss = 0.90467386\n",
      "Iteration 6, loss = 0.83971021\n",
      "Iteration 7, loss = 0.80657177\n",
      "Iteration 8, loss = 0.78652692\n",
      "Iteration 9, loss = 0.76760132\n",
      "Iteration 10, loss = 0.74839446\n",
      "Iteration 11, loss = 0.74797964\n",
      "Iteration 12, loss = 0.73603652\n",
      "Iteration 13, loss = 0.72608540\n",
      "Iteration 14, loss = 0.72080346\n",
      "Iteration 15, loss = 0.72336314\n",
      "Iteration 16, loss = 0.72521637\n",
      "Iteration 17, loss = 0.72841585\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "La précision sur les données d'entrainement est :  0.9621828626136908\n",
      "La précision sur les données de validation est :  0.7790530846484935\n",
      "La précision sur les données de test est :  0.7586206896551724\n"
     ]
    }
   ],
   "source": [
    "mlp_clf = MLPClassifier(activation='relu', alpha=1.0, verbose=2, batch_size=50)\n",
    "mlp_clf.fit(X_train_counts, y_train)\n",
    "\n",
    "pred_train_mlp = mlp_clf.predict(X_train_counts)\n",
    "pred_dev_mlp = mlp_clf.predict(X_dev_counts)\n",
    "pred_test_mlp = mlp_clf.predict(X_test_counts)\n",
    "\n",
    "print(\"La précision sur les données d'entrainement est : \", metrics.accuracy_score(y_train, pred_train_mlp))\n",
    "print(\"La précision sur les données de validation est : \", metrics.accuracy_score(y_dev, pred_dev_mlp))\n",
    "print(\"La précision sur les données de test est : \", metrics.accuracy_score(y_test, pred_test_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-Analyse des Performances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------  Naive Bayse Bag Of  Word  ------------------\n",
      "\n",
      "La précision sur les données d'entrainement est :  0.8353279080899952\n",
      "La précision sur les données de validation est :  0.7101865136298422\n",
      "La précision sur les données de test est :  0.7140804597701149\n",
      "\n",
      "\n",
      " Matrice de Classification \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Advertisement       0.64      0.60      0.62        47\n",
      "        Email       0.93      0.94      0.94       120\n",
      "         Form       0.70      0.82      0.75        76\n",
      "       Letter       0.74      0.61      0.67       132\n",
      "         Memo       0.63      0.75      0.68       112\n",
      "         News       0.62      0.79      0.70        38\n",
      "         Note       0.44      0.28      0.34        39\n",
      "       Report       0.51      0.54      0.53        48\n",
      "       Resume       1.00      0.95      0.98        21\n",
      "   Scientific       0.78      0.67      0.72        63\n",
      "\n",
      "  avg / total       0.72      0.71      0.71       696\n",
      "\n",
      "Matrice de Confusion Bag Of Word\n",
      "[[ 28   0   6   0   5   3   4   1   0   0]\n",
      " [  0 113   0   3   3   1   0   0   0   0]\n",
      " [  5   0  62   1   3   0   5   0   0   0]\n",
      " [  0   2   2  81  22   7   1  15   0   2]\n",
      " [  2   3   1  12  84   1   1   7   0   1]\n",
      " [  5   0   0   0   0  30   2   0   0   1]\n",
      " [  2   3   7   6   9   0  11   0   0   1]\n",
      " [  0   0   1   6   5   3   0  26   0   7]\n",
      " [  0   0   0   0   0   0   1   0  20   0]\n",
      " [  2   0  10   1   3   3   0   2   0  42]]\n"
     ]
    }
   ],
   "source": [
    "print('------------------  Naive Bayse Bag Of  Word  ------------------\\n')\n",
    "print(\"La précision sur les données d'entrainement est : \", metrics.accuracy_score(y_train, pred_train))\n",
    "print(\"La précision sur les données de validation est : \", metrics.accuracy_score(y_dev, pred_dev))\n",
    "print(\"La précision sur les données de test est : \", metrics.accuracy_score(y_test, pred_test))\n",
    "print('\\n\\n Matrice de Classification \\n')\n",
    "print(classification_report(y_test, pred_test))\n",
    "print('Matrice de Confusion Bag Of Word')\n",
    "print(confusion_matrix(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- TF-IDF -----------------------\n",
      "\n",
      "La précision sur les données d'entrainement es:  0.7415031115366204\n",
      "La précision sur les données de validation est :  0.6556671449067432\n",
      "La précision sur les données de test est :  0.6422413793103449\n",
      "\n",
      "\n",
      " Matrice de Classification \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Advertisement       1.00      0.21      0.35        47\n",
      "        Email       0.90      0.94      0.92       120\n",
      "         Form       0.58      0.84      0.68        76\n",
      "       Letter       0.56      0.67      0.61       132\n",
      "         Memo       0.46      0.86      0.60       112\n",
      "         News       0.91      0.53      0.67        38\n",
      "         Note       0.00      0.00      0.00        39\n",
      "       Report       0.56      0.10      0.18        48\n",
      "       Resume       1.00      0.90      0.95        21\n",
      "   Scientific       0.89      0.51      0.65        63\n",
      "\n",
      "  avg / total       0.67      0.64      0.61       696\n",
      "\n",
      "\n",
      "Matrice de Confusion TF-IDF\n",
      "[[ 10   1  16   8  12   0   0   0   0   0]\n",
      " [  0 113   0   3   4   0   0   0   0   0]\n",
      " [  0   2  64   1   9   0   0   0   0   0]\n",
      " [  0   1   0  88  41   0   0   1   0   1]\n",
      " [  0   3   1  12  96   0   0   0   0   0]\n",
      " [  0   1   5  10   0  20   0   2   0   0]\n",
      " [  0   4  13  10  11   0   0   0   0   1]\n",
      " [  0   0   0  17  23   1   0   5   0   2]\n",
      " [  0   0   1   1   0   0   0   0  19   0]\n",
      " [  0   0  11   7  11   1   0   1   0  32]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print('---------------------- TF-IDF -----------------------\\n')\n",
    "print(\"La précision sur les données d'entrainement es: \", metrics.accuracy_score(y_train, pred_train_tf))\n",
    "print(\"La précision sur les données de validation est : \", metrics.accuracy_score(y_dev, pred_dev_tf))\n",
    "print(\"La précision sur les données de test est : \", metrics.accuracy_score(y_test, pred_test_tf))\n",
    "print('\\n\\n Matrice de Classification \\n')\n",
    "print(classification_report(y_test, pred_test_tf))\n",
    "print()\n",
    "print('Matrice de Confusion TF-IDF')\n",
    "print(confusion_matrix(y_test, pred_test_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- MLP Classifier with TF-IDF representation -----------------------\n",
      "\n",
      "La précision sur les données d'entrainement est :  0.9621828626136908\n",
      "La précision sur les données de validation est :  0.7790530846484935\n",
      "La précision sur les données de test est :  0.7586206896551724\n",
      "\n",
      "\n",
      " Matrice de Classification \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Advertisement       0.80      0.43      0.56        47\n",
      "        Email       0.92      0.97      0.94       120\n",
      "         Form       0.72      0.89      0.80        76\n",
      "       Letter       0.78      0.69      0.73       132\n",
      "         Memo       0.76      0.82      0.79       112\n",
      "         News       0.78      0.76      0.77        38\n",
      "         Note       0.50      0.82      0.62        39\n",
      "       Report       0.48      0.48      0.48        48\n",
      "       Resume       1.00      0.95      0.98        21\n",
      "   Scientific       0.84      0.59      0.69        63\n",
      "\n",
      "  avg / total       0.77      0.76      0.76       696\n",
      "\n",
      "\n",
      "Matrice de Confusion MLP-BoW\n",
      "[[ 20   0   2   1   0   0  23   1   0   0]\n",
      " [  0 116   0   2   1   0   0   1   0   0]\n",
      " [  0   0  68   2   1   0   5   0   0   0]\n",
      " [  0   6   2  91  21   3   0   8   0   1]\n",
      " [  0   2   1   6  92   1   2   8   0   0]\n",
      " [  4   1   1   0   0  29   0   3   0   0]\n",
      " [  0   0   5   1   0   0  32   0   0   1]\n",
      " [  0   1   3  12   3   1   0  23   0   5]\n",
      " [  0   0   1   0   0   0   0   0  20   0]\n",
      " [  1   0  12   1   3   3   2   4   0  37]]\n"
     ]
    }
   ],
   "source": [
    "print('---------------------- MLP Classifier with TF-IDF representation -----------------------\\n')\n",
    "\n",
    "print(\"La précision sur les données d'entrainement est : \", metrics.accuracy_score(y_train, pred_train_mlp))\n",
    "print(\"La précision sur les données de validation est : \", metrics.accuracy_score(y_dev, pred_dev_mlp))\n",
    "print(\"La précision sur les données de test est : \", metrics.accuracy_score(y_test, pred_test_mlp))\n",
    "print('\\n\\n Matrice de Classification \\n')\n",
    "print(classification_report(y_test, pred_test_mlp))\n",
    "print()\n",
    "print('Matrice de Confusion MLP-BoW')\n",
    "print(confusion_matrix(y_test, pred_test_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce travail j'ai comparé entre deux methodes, la première cosistait à entrainer un classifieur bayésien naif, sur des données transformées par un Bag Of Word, et un TF-IDF, la deuxième methode consiste à utiliser un réseau de neurones dans mon cas un MLP, pour classer les meme donnnées. l'utilisation d'un classifieur bayesien,est  réputé pour etre non performant dans le cas où les mots son dépendant entre eux, mais puisque c'est simple à implémenter et rapide à exécuter j'ai choisie de le tester, et puis le comparer avec un reseau de neurone qui sont réputés pour etre beaucoup plus raubuste et voir ce que ça donne.\n",
    "\n",
    "J'ai remarque que le modèle bayèsien nous donne une précision de 71% dans le cas où les données sont transformées avec un bag of word, et 64% dans le cas où il sont transformées avec un TF-IDF\n",
    "Tandis qu'avec le reseau de neurones la précision sur les données de test est de 75%.\n",
    "En regardant la matrice de Confusion des deux approches, on remarque que la méthode bayésiène n'est pas équilibré et de nombreuses classes sont confondus, contairement à la matrice du reseau de neurone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pistes d'améliorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l'idée que j'ai eu et que j'ai appliqué durant ce travail et de combiner les approches, mais on pourrait aller plus loin, puisque de nos jours les CNN, sont entrain d'ateindres des résultats trés satisfaisants,en terme de reconnaissance d'image, et puisque on à déjas les images des documents scanés, on aura pas besoin du texte obtenu à l'aide d'un OCR et qui contient des erreurs d'interprétation, on donne en entrée à un reseau de neurones à convolution, nos images de documents qui va classer automatiquement, seul inconvénient c'est qu'il faut avoir une grande base de données d'apprentissage étiquettée. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
